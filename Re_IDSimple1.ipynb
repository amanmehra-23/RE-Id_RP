{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanmehra-23/RE-Id_RP/blob/main/Re_IDSimple1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w03GuW5qg4qg",
        "outputId": "a8a0603a-1027-4c4e-cab8-19f7e6bd9896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torchvision import models, transforms\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "E1s3Hgakgva3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 1. Define the Dataset Class\n",
        "# --------------------------\n",
        "class Market1501Dataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Directory with images (e.g., bounding_box_train, bounding_box_test, or query).\n",
        "            transform: Preprocessing transforms.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        for file in os.listdir(root_dir):\n",
        "            if file.endswith('.jpg'):\n",
        "                # Expected filename: \"0002_c1s1_000451_03.jpg\"\n",
        "                id_str = file.split('_')[0]\n",
        "                if id_str.startswith('-') or not id_str.isdigit():\n",
        "                    continue  # Skip junk/distractor images\n",
        "                person_id = int(id_str)\n",
        "                if person_id <= 0:\n",
        "                    continue\n",
        "                self.image_paths.append(os.path.join(root_dir, file))\n",
        "                self.labels.append(person_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "kAALO4izgv5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EfficientNet-B4 Backbone (removes the classification head)\n",
        "class EfficientNetBackbone(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(EfficientNetBackbone, self).__init__()\n",
        "        effnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "        self.features = nn.Sequential(\n",
        "            effnet.features  # This includes all convolutional layers before avgpool and fc\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Expected output: (B, 1792, 7, 7) for 224x224 input\n",
        "        return self.features(x)\n"
      ],
      "metadata": {
        "id": "whR2yAJkhSGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Grid Graph for the feature map from the CNN\n",
        "def build_grid_edge_index(grid_size):\n",
        "    \"\"\"\n",
        "    Constructs edge indices for a grid graph.\n",
        "    Each node (patch) is connected to its right and down neighbors (and vice versa).\n",
        "    \"\"\"\n",
        "    H, W = grid_size\n",
        "    edges = []\n",
        "    for i in range(H):\n",
        "        for j in range(W):\n",
        "            idx = i * W + j\n",
        "            # Connect to right neighbor, if available\n",
        "            if j + 1 < W:\n",
        "                right_idx = i * W + (j + 1)\n",
        "                edges.append((idx, right_idx))\n",
        "                edges.append((right_idx, idx))\n",
        "            # Connect to down neighbor, if available\n",
        "            if i + 1 < H:\n",
        "                down_idx = (i + 1) * W + j\n",
        "                edges.append((idx, down_idx))\n",
        "                edges.append((down_idx, idx))\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    return edge_index\n"
      ],
      "metadata": {
        "id": "dFhxAUZ_hVci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GNN Branch that processes the CNN feature map\n",
        "class GNNBranch(nn.Module):\n",
        "    def __init__(self, in_channels=2048, hidden_channels=512, out_channels=256, grid_size=(7,7)):\n",
        "        super(GNNBranch, self).__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.edge_index = build_grid_edge_index(grid_size)  # fixed graph structure\n",
        "\n",
        "        # Two GCN layers\n",
        "        self.gcn1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.gcn2 = GCNConv(hidden_channels, out_channels)\n",
        "        # An additional projection layer for refinement (optional)\n",
        "        self.fc = nn.Linear(out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Feature map of shape (B, 2048, 7, 7).\n",
        "        Returns:\n",
        "            Embedding tensor of shape (B, out_channels) [e.g., (B, 256)].\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        N = H * W  # number of nodes, e.g., 7*7 = 49\n",
        "        # Reshape feature map to nodes: (B, N, C)\n",
        "        x_nodes = x.view(B, C, N).permute(0, 2, 1)\n",
        "        embeddings = []\n",
        "        edge_index = self.edge_index.to(x.device)\n",
        "        for i in range(B):\n",
        "            node_feat = x_nodes[i]  # shape (N, C)\n",
        "            h = F.relu(self.gcn1(node_feat, edge_index))\n",
        "            h = self.gcn2(h, edge_index)\n",
        "            pooled = h.mean(dim=0)  # global mean pooling (N, ) -> (C_out,)\n",
        "            embeddings.append(pooled)\n",
        "        embeddings = torch.stack(embeddings, dim=0)\n",
        "        embeddings = self.fc(embeddings)\n",
        "        return embeddings  # shape (B, out_channels)\n"
      ],
      "metadata": {
        "id": "1MaLnpiQhXvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleReIDModel(nn.Module):\n",
        "    def __init__(self, device='cuda'):\n",
        "        super(SimpleReIDModel, self).__init__()\n",
        "        self.device = device\n",
        "        self.backbone = EfficientNetBackbone(pretrained=True)\n",
        "        self.gnn_branch = GNNBranch(in_channels=1280, hidden_channels=512, out_channels=256, grid_size=(7,7))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.device)\n",
        "        feature_map = self.backbone(x)  # shape (B, 1792, 7, 7)\n",
        "        embedding = self.gnn_branch(feature_map)\n",
        "        return embedding\n"
      ],
      "metadata": {
        "id": "Eekhq0-2hb-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 4. Define the Supervised Contrastive Loss\n",
        "# --------------------------\n",
        "def supervised_contrastive_loss(embeddings, labels, temperature=0.07):\n",
        "    \"\"\"\n",
        "    Computes the supervised contrastive loss. Embeddings with the same label are pulled together,\n",
        "    while embeddings with different labels are pushed apart.\n",
        "\n",
        "    Args:\n",
        "        embeddings: Tensor of shape (B, D) [B: batch size, D: embedding dimension].\n",
        "        labels: Tensor of shape (B,) with the identity labels.\n",
        "        temperature: Scaling factor for the logits.\n",
        "    Returns:\n",
        "        A scalar loss value.\n",
        "    \"\"\"\n",
        "    device = embeddings.device\n",
        "    batch_size = embeddings.shape[0]\n",
        "\n",
        "    # Normalize embeddings to unit norm\n",
        "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "    # Compute cosine similarity matrix, shape: (B, B)\n",
        "    similarity_matrix = torch.matmul(embeddings, embeddings.T)\n",
        "    logits = similarity_matrix / temperature\n",
        "\n",
        "    # Create a binary mask where mask[i, j]=1 if labels[i]==labels[j] and i != j\n",
        "    labels = labels.contiguous().view(-1, 1)\n",
        "    mask = torch.eq(labels, labels.T).float().to(device)\n",
        "    logits_mask = torch.ones_like(mask) - torch.eye(batch_size, device=device)\n",
        "    mask = mask * logits_mask\n",
        "\n",
        "    # Calculate log probabilities\n",
        "    exp_logits = torch.exp(logits) * logits_mask\n",
        "    denominator = exp_logits.sum(1, keepdim=True) + 1e-8\n",
        "    log_prob = logits - torch.log(denominator)\n",
        "\n",
        "    mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-8)\n",
        "    loss = -mean_log_prob_pos.mean()\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "sBz6S4QahePo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"pengcw1/market-1501\")\n",
        "print(\"Original path:\", path)\n",
        "\n",
        "# Desired destination directory in your Colab workspace\n",
        "destination_path = \"/content/data/market-1501\"\n",
        "\n",
        "# Move the dataset to your preferred path\n",
        "if not os.path.exists(destination_path):\n",
        "    shutil.copytree(path, destination_path)\n",
        "\n",
        "print(\"✅ Dataset moved to:\", destination_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VbjPJ3PhmuL",
        "outputId": "ff7f7807-f5f6-41cc-cf33-23cab8fcd70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original path: /kaggle/input/market-1501\n",
            "✅ Dataset moved to: /content/data/market-1501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 5. Setup the Dataset and DataLoader\n",
        "# --------------------------\n",
        "# Define a preprocessing pipeline (same for training and evaluation)\n",
        "preprocess_pipeline = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# Set your dataset paths (adjust these based on your environment)\n",
        "# For example, assume you have a folder structure with \"bounding_box_train\" for training.\n",
        "dataset_path = \"/kaggle/input/market-1501/Market-1501-v15.09.15\"  # Adjust if necessary\n",
        "train_dir = os.path.join(dataset_path, \"bounding_box_train\")\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "train_dataset = Market1501Dataset(root_dir=train_dir, transform=preprocess_pipeline)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n"
      ],
      "metadata": {
        "id": "xxRRywlVhhuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 6. Training Loop\n",
        "# --------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SimpleReIDModel(device=device).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "num_epochs = 10  # Set as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        embeddings = model(images)  # (B, 256)\n",
        "        loss = supervised_contrastive_loss(embeddings, labels, temperature=0.07)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Supervised Contrastive Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Optionally, save your model once training is complete:\n",
        "torch.save(model.state_dict(), \"/content/simple_reid_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3SOdmORhkyz",
        "outputId": "333064bb-6a40-4b87-cc5c-928a1d6cb7fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Supervised Contrastive Loss: 0.1297\n",
            "Epoch 2/10 - Supervised Contrastive Loss: 0.0904\n",
            "Epoch 3/10 - Supervised Contrastive Loss: 0.0849\n",
            "Epoch 4/10 - Supervised Contrastive Loss: 0.0644\n",
            "Epoch 5/10 - Supervised Contrastive Loss: 0.0469\n",
            "Epoch 6/10 - Supervised Contrastive Loss: 0.0423\n",
            "Epoch 7/10 - Supervised Contrastive Loss: 0.0431\n",
            "Epoch 8/10 - Supervised Contrastive Loss: 0.0419\n",
            "Epoch 9/10 - Supervised Contrastive Loss: 0.0386\n",
            "Epoch 10/10 - Supervised Contrastive Loss: 0.0366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 7. (Optional) Evaluation Functions\n",
        "# --------------------------\n",
        "def extract_embeddings(model, data_loader, device):\n",
        "    model.eval()\n",
        "    all_embeddings = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            embeddings = model(images)\n",
        "            all_embeddings.append(embeddings.cpu())\n",
        "            all_labels.extend(labels.numpy())\n",
        "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "    return all_embeddings, np.array(all_labels)\n",
        "\n",
        "def compute_distance_matrix(query_emb, gallery_emb, metric='euclidean'):\n",
        "    if metric == 'euclidean':\n",
        "        return torch.cdist(query_emb, gallery_emb, p=2)\n",
        "    elif metric == 'cosine':\n",
        "        q_norm = F.normalize(query_emb, p=2, dim=1)\n",
        "        g_norm = F.normalize(gallery_emb, p=2, dim=1)\n",
        "        return 1 - torch.mm(q_norm, g_norm.t())\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported metric\")\n",
        "\n",
        "def evaluate_rank1_map(dist_matrix, query_labels, gallery_labels):\n",
        "    num_queries = dist_matrix.size(0)\n",
        "    rank1 = 0\n",
        "    ap_list = []\n",
        "    query_labels = np.array(query_labels)\n",
        "    gallery_labels = np.array(gallery_labels)\n",
        "    for i in range(num_queries):\n",
        "        distances = dist_matrix[i].cpu().numpy()\n",
        "        sorted_indices = np.argsort(distances)\n",
        "        matches = (gallery_labels[sorted_indices] == query_labels[i])\n",
        "        if matches[0]:\n",
        "            rank1 += 1\n",
        "        num_relevant = matches.sum()\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        precisions = []\n",
        "        correct = 0\n",
        "        for j, flag in enumerate(matches):\n",
        "            if flag:\n",
        "                correct += 1\n",
        "                precisions.append(correct / (j + 1))\n",
        "        ap_list.append(np.mean(precisions))\n",
        "    rank1_accuracy = rank1 / num_queries\n",
        "    mAP = np.mean(ap_list) if ap_list else 0\n",
        "    return rank1_accuracy, mAP\n"
      ],
      "metadata": {
        "id": "zRYakPZHiyDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"pengcw1/market-1501\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "RYo3mW6OjMBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --------------------------\n",
        "# Define the Evaluation Dataset (if not already defined)\n",
        "# --------------------------\n",
        "class Market1501Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Directory with images (e.g., query, bounding_box_test).\n",
        "            transform: Preprocessing transforms.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        for file in os.listdir(root_dir):\n",
        "            if file.endswith('.jpg'):\n",
        "                id_str = file.split('_')[0]\n",
        "                # Skip distractors/junk images (filenames starting with '-' or non-digit)\n",
        "                if id_str.startswith('-') or not id_str.isdigit():\n",
        "                    continue\n",
        "                person_id = int(id_str)\n",
        "                if person_id <= 0:\n",
        "                    continue\n",
        "                self.image_paths.append(os.path.join(root_dir, file))\n",
        "                self.labels.append(person_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# --------------------------\n",
        "# Evaluation Functions (as provided earlier)\n",
        "# --------------------------\n",
        "def extract_embeddings(model, data_loader, device):\n",
        "    model.eval()\n",
        "    all_embeddings = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            embeddings = model(images)  # Expected shape: (B, 256)\n",
        "            all_embeddings.append(embeddings.cpu())\n",
        "            all_labels.extend(labels.numpy())\n",
        "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "    return all_embeddings, np.array(all_labels)\n",
        "\n",
        "def compute_distance_matrix(query_emb, gallery_emb, metric='euclidean'):\n",
        "    if metric == 'euclidean':\n",
        "        return torch.cdist(query_emb, gallery_emb, p=2)\n",
        "    elif metric == 'cosine':\n",
        "        q_norm = F.normalize(query_emb, p=2, dim=1)\n",
        "        g_norm = F.normalize(gallery_emb, p=2, dim=1)\n",
        "        return 1 - torch.mm(q_norm, g_norm.t())\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported metric\")\n",
        "\n",
        "def evaluate_rank1_map(dist_matrix, query_labels, gallery_labels):\n",
        "    num_queries = dist_matrix.size(0)\n",
        "    rank1 = 0\n",
        "    ap_list = []\n",
        "    query_labels = np.array(query_labels)\n",
        "    gallery_labels = np.array(gallery_labels)\n",
        "    for i in range(num_queries):\n",
        "        distances = dist_matrix[i].cpu().numpy()\n",
        "        sorted_indices = np.argsort(distances)\n",
        "        matches = (gallery_labels[sorted_indices] == query_labels[i])\n",
        "        if matches[0]:\n",
        "            rank1 += 1\n",
        "        num_relevant = matches.sum()\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        precisions = []\n",
        "        correct = 0\n",
        "        for j, flag in enumerate(matches):\n",
        "            if flag:\n",
        "                correct += 1\n",
        "                precisions.append(correct / (j + 1))\n",
        "        ap_list.append(np.mean(precisions))\n",
        "    rank1_accuracy = rank1 / num_queries\n",
        "    mAP = np.mean(ap_list) if ap_list else 0\n",
        "    return rank1_accuracy, mAP\n",
        "\n",
        "# --------------------------\n",
        "# Set Up Preprocessing Pipeline\n",
        "# --------------------------\n",
        "preprocess_pipeline = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
        "        std=[0.229, 0.224, 0.225]    # ImageNet std\n",
        "    )\n",
        "])\n",
        "\n",
        "# --------------------------\n",
        "# Set Dataset Paths (Adjust based on your downloaded structure)\n",
        "# --------------------------\n",
        "dataset_path = \"/kaggle/input/market-1501/Market-1501-v15.09.15\"  # Change as needed\n",
        "query_dir = os.path.join(dataset_path, \"query\")\n",
        "gallery_dir = os.path.join(dataset_path, \"bounding_box_test\")\n",
        "\n",
        "# --------------------------\n",
        "# Create DataLoaders for Query and Gallery Sets\n",
        "# --------------------------\n",
        "query_dataset = Market1501Dataset(root_dir=query_dir, transform=preprocess_pipeline)\n",
        "gallery_dataset = Market1501Dataset(root_dir=gallery_dir, transform=preprocess_pipeline)\n",
        "\n",
        "query_loader = DataLoader(query_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "gallery_loader = DataLoader(gallery_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# --------------------------\n",
        "# Load the Saved Model\n",
        "# --------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Instantiate your model. For simplicity, this version uses only the ResNet + GNN branch.\n",
        "# Make sure SimpleReIDModel is defined as in the previous code.\n",
        "class SimpleReIDModel(nn.Module):\n",
        "    def __init__(self, device='cuda'):\n",
        "        super(SimpleReIDModel, self).__init__()\n",
        "        # Assuming ResNetBackbone and GNNBranch are defined\n",
        "        self.backbone = EfficientNetBackbone(pretrained=True)\n",
        "        self.gnn_branch = GNNBranch(in_channels=1280, hidden_channels=512, out_channels=256, grid_size=(7,7))\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.device)\n",
        "        feature_map = self.backbone(x)  # Shape: (B, 2048, 7, 7)\n",
        "        embedding = self.gnn_branch(feature_map)  # Shape: (B, 256)\n",
        "        return embedding\n",
        "\n",
        "# Instantiate and load your saved model\n",
        "model = SimpleReIDModel(device=device).to(device)\n",
        "model_checkpoint = \"/content/simple_reid_model.pth\"  # Adjust path if needed\n",
        "model.load_state_dict(torch.load(model_checkpoint, map_location=device))\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# --------------------------\n",
        "# Run Evaluation\n",
        "# --------------------------\n",
        "# 1. Extract embeddings for query and gallery sets\n",
        "query_embeddings, query_labels = extract_embeddings(model, query_loader, device)\n",
        "gallery_embeddings, gallery_labels = extract_embeddings(model, gallery_loader, device)\n",
        "\n",
        "# 2. Compute the distance matrix (choose metric: 'euclidean' or 'cosine')\n",
        "dist_matrix = compute_distance_matrix(query_embeddings, gallery_embeddings, metric='euclidean')\n",
        "\n",
        "# 3. Evaluate Rank-1 and mAP\n",
        "rank1_accuracy, mAP = evaluate_rank1_map(dist_matrix, query_labels, gallery_labels)\n",
        "print(\"Rank-1 Accuracy: {:.2%}\".format(rank1_accuracy))\n",
        "print(\"mAP: {:.2%}\".format(mAP))\n"
      ],
      "metadata": {
        "id": "HdGM8HEupJDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b0471e2-b1d2-4544-9c05-b6277f408c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank-1 Accuracy: 91.86%\n",
            "mAP: 35.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nLrBQEPzmJT-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}