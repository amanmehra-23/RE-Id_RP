{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOISrNFUWgEgV4nvfKNQPck",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanmehra-23/RE-Id_RP/blob/main/RE_IDPray.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric openai-clip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6T-8-dCO3eZ",
        "outputId": "13f4dc49-7650-4ac6-cfc7-1d210a87e092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Collecting openai-clip\n",
            "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Collecting ftfy (from openai-clip)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from openai-clip) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->openai-clip) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-clip\n",
            "  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=c00d8a57e85c450a7b91e20586c1a41d6eaf97644cc81db25719351c4cbf975a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/17/90/042948fd2e2a87f1dcf6db6d438cad015c49db0c53d1d9c7dc\n",
            "Successfully built openai-clip\n",
            "Installing collected packages: ftfy, openai-clip\n",
            "Successfully installed ftfy-6.3.1 openai-clip-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Nxwe2aQe65d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 1: ResNet-50 Backbone ---\n",
        "class ResNetBackbone(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(ResNetBackbone, self).__init__()\n",
        "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "        # Remove the final pooling and FC layers: output shape (B, 2048, 7, 7)\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)  # Expected shape: (B, 2048, 7, 7)\n"
      ],
      "metadata": {
        "id": "Stq5QIQ8f6hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 2: Build Grid Graph ---\n",
        "def build_grid_edge_index(grid_size):\n",
        "    \"\"\"\n",
        "    Constructs edge indices for a grid graph given grid dimensions.\n",
        "    Each node (patch) is connected to its right and down neighbor (and vice versa).\n",
        "    \"\"\"\n",
        "    H, W = grid_size\n",
        "    edges = []\n",
        "    for i in range(H):\n",
        "        for j in range(W):\n",
        "            idx = i * W + j\n",
        "            # Connect to right neighbor if exists\n",
        "            if j + 1 < W:\n",
        "                right_idx = i * W + (j + 1)\n",
        "                edges.append((idx, right_idx))\n",
        "                edges.append((right_idx, idx))\n",
        "            # Connect to down neighbor if exists\n",
        "            if i + 1 < H:\n",
        "                down_idx = (i + 1) * W + j\n",
        "                edges.append((idx, down_idx))\n",
        "                edges.append((down_idx, idx))\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    return edge_index  # Shape: (2, num_edges)"
      ],
      "metadata": {
        "id": "X23V1eXQnzV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 3: GNN Branch ---\n",
        "class GNNBranch(nn.Module):\n",
        "    def __init__(self, in_channels=2048, hidden_channels=512, out_channels=256, grid_size=(7,7)):\n",
        "        super(GNNBranch, self).__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.edge_index = build_grid_edge_index(grid_size)  # Fixed for a given grid size\n",
        "\n",
        "        # Two GCN layers\n",
        "        self.gcn1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.gcn2 = GCNConv(hidden_channels, out_channels)\n",
        "        # Optional FC layer for further refinement\n",
        "        self.fc = nn.Linear(out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: CNN feature map of shape (B, 2048, H, W) with H,W = grid_size (e.g., 7,7)\n",
        "        Returns:\n",
        "            A tensor of shape (B, out_channels) representing the person embedding.\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        N = H * W  # Number of nodes (e.g., 49)\n",
        "        # Reshape: (B, C, H, W) -> (B, N, C)\n",
        "        x_nodes = x.view(B, C, N).permute(0, 2, 1)  # (B, N, 2048)\n",
        "        embeddings = []\n",
        "        edge_index = self.edge_index.to(x.device)  # Ensure edge_index is on the same device\n",
        "        for i in range(B):\n",
        "            node_feat = x_nodes[i]  # (N, 2048)\n",
        "            h = F.relu(self.gcn1(node_feat, edge_index))  # (N, hidden_channels)\n",
        "            h = self.gcn2(h, edge_index)  # (N, out_channels)\n",
        "            # Global mean pooling: average over the N nodes\n",
        "            pooled = h.mean(dim=0)  # (out_channels,)\n",
        "            embeddings.append(pooled)\n",
        "        embeddings = torch.stack(embeddings, dim=0)  # (B, out_channels)\n",
        "        embeddings = self.fc(embeddings)\n",
        "        return embeddings  # (B, out_channels) e.g., (B, 256)\n"
      ],
      "metadata": {
        "id": "WB40z8acn4uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 4: CLIP Branch ---\n",
        "# For the CLIP branch, we use OpenAI's CLIP model.\n",
        "# Ensure you have installed the clip package (e.g., pip install git+https://github.com/openai/CLIP.git)\n",
        "import clip\n",
        "\n",
        "class CLIPBranch(nn.Module):\n",
        "    def __init__(self, device='cuda'):\n",
        "        super(CLIPBranch, self).__init__()\n",
        "        self.clip_model, self.clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "        self.clip_model.eval()  # Set to eval mode\n",
        "        self.proj = nn.Linear(512, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure x is on the correct device\n",
        "        x = x.to(next(self.clip_model.parameters()).device)\n",
        "        with torch.no_grad():\n",
        "            clip_emb = self.clip_model.encode_image(x)  # (B, 512)\n",
        "        # Convert to float32 to match the projection layer parameters\n",
        "        clip_emb = clip_emb.float()\n",
        "        clip_emb = self.proj(clip_emb)  # (B, 256)\n",
        "        return clip_emb\n",
        "\n"
      ],
      "metadata": {
        "id": "rpjk3_hKn-EV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 5: Fusion Module ---\n",
        "class FusionModule(nn.Module):\n",
        "    def __init__(self, emb_dim=256):\n",
        "        super(FusionModule, self).__init__()\n",
        "        # Fusion via concatenation then projection to emb_dim\n",
        "        self.fc = nn.Linear(emb_dim * 2, emb_dim)\n",
        "\n",
        "    def forward(self, gnn_emb, clip_emb):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            gnn_emb: Embedding from GNN branch (B, emb_dim)\n",
        "            clip_emb: Embedding from CLIP branch (B, emb_dim)\n",
        "        Returns:\n",
        "            Fused embedding (B, emb_dim)\n",
        "        \"\"\"\n",
        "        fused = torch.cat([gnn_emb, clip_emb], dim=1)  # (B, 2*emb_dim)\n",
        "        fused = self.fc(fused)\n",
        "        return fused"
      ],
      "metadata": {
        "id": "Bzg5nxiGoHcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 6: Combined Model ---\n",
        "class ReIDMultimodalNet(nn.Module):\n",
        "    def __init__(self, device='cuda'):\n",
        "        super(ReIDMultimodalNet, self).__init__()\n",
        "        self.device = device\n",
        "        self.backbone = ResNetBackbone(pretrained=True)\n",
        "        self.gnn_branch = GNNBranch(in_channels=2048, hidden_channels=512, out_channels=256, grid_size=(7,7))\n",
        "        self.clip_branch = CLIPBranch(device=device)\n",
        "        self.fusion = FusionModule(emb_dim=256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input image tensor of shape (B, 3, 224, 224)\n",
        "        Returns:\n",
        "            Fused multimodal embedding (B, 256)\n",
        "        \"\"\"\n",
        "        x = x.to(self.device)\n",
        "        # CNN backbone to get feature map: (B, 2048, 7, 7)\n",
        "        feature_map = self.backbone(x)\n",
        "        # GNN branch: process feature map and produce a 256-D embedding\n",
        "        gnn_emb = self.gnn_branch(feature_map)\n",
        "        # CLIP branch: process the image and produce a 256-D embedding\n",
        "        clip_emb = self.clip_branch(x)\n",
        "        # Fusion: combine the two embeddings\n",
        "        fused_emb = self.fusion(gnn_emb, clip_emb)\n",
        "        return fused_emb"
      ],
      "metadata": {
        "id": "-V1d3Yg5oK51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Preprocessing Pipeline for Input Images ---\n",
        "# Standard preprocessing for ResNet and CLIP (CLIP's preprocessing may differ slightly)\n",
        "preprocess_pipeline = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
        "        std=[0.229, 0.224, 0.225]    # ImageNet std\n",
        "    )\n",
        "])\n",
        "\n",
        "def load_image(image_path_or_url):\n",
        "    if image_path_or_url.startswith(\"http\"):\n",
        "        response = requests.get(image_path_or_url)\n",
        "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    else:\n",
        "        image = Image.open(image_path_or_url).convert(\"RGB\")\n",
        "    return image\n",
        "\n",
        "# --- Testing the Full Pipeline ---\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = ReIDMultimodalNet(device=device).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess an example image\n",
        "    image_url = \"https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg\"\n",
        "    image = load_image(image_url)\n",
        "    input_tensor = preprocess_pipeline(image).unsqueeze(0)  # (1, 3, 224, 224)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fused_embedding = model(input_tensor)\n",
        "    print(\"Fused multimodal embedding shape:\", fused_embedding.shape)  # Expected: (1, 256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKaRHpLfoQfX",
        "outputId": "953650f1-26f8-445b-9bf3-a887dae58423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 144MB/s]\n",
            "100%|███████████████████████████████████████| 338M/338M [00:06<00:00, 54.4MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fused multimodal embedding shape: torch.Size([1, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"pengcw1/market-1501\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPsr-WfTqYoZ",
        "outputId": "d82ae082-9333-47bf-f39a-66beba7137f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/market-1501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def supervised_contrastive_loss(embeddings, labels, temperature=0.07):\n",
        "    \"\"\"\n",
        "    Computes the supervised contrastive loss as in Khosla et al. (2020).\n",
        "\n",
        "    Args:\n",
        "        embeddings: Tensor of shape (B, D) where B is the batch size and D is embedding dimension.\n",
        "        labels: Tensor of shape (B,) with integer labels.\n",
        "        temperature: A scaling factor for the logits.\n",
        "    Returns:\n",
        "        loss: A scalar representing the supervised contrastive loss.\n",
        "    \"\"\"\n",
        "    device = embeddings.device\n",
        "    batch_size = embeddings.shape[0]\n",
        "\n",
        "    # Normalize embeddings to have unit norm\n",
        "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "    # Compute cosine similarity matrix (B, B) and scale by temperature\n",
        "    similarity_matrix = torch.matmul(embeddings, embeddings.T)\n",
        "    logits = similarity_matrix / temperature\n",
        "\n",
        "    # Create mask where mask[i, j] = 1 if samples i and j have the same label and i != j\n",
        "    labels = labels.contiguous().view(-1, 1)\n",
        "    mask = torch.eq(labels, labels.T).float().to(device)\n",
        "\n",
        "    # Exclude self-comparisons from both the mask and the denominator\n",
        "    logits_mask = torch.ones_like(mask) - torch.eye(batch_size, device=device)\n",
        "    mask = mask * logits_mask\n",
        "\n",
        "    # Compute log probabilities for each pair\n",
        "    exp_logits = torch.exp(logits) * logits_mask  # (B, B)\n",
        "    denominator = exp_logits.sum(1, keepdim=True) + 1e-8  # To avoid division by zero\n",
        "    log_prob = logits - torch.log(denominator)\n",
        "\n",
        "    # For each sample, calculate mean log-likelihood over all positive pairs\n",
        "    mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-8)\n",
        "\n",
        "    # The loss is the negative average of these log-likelihoods\n",
        "    loss = -mean_log_prob_pos.mean()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ux6j3E-eORRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class Market1501Dataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Directory with all images (e.g., bounding_box_train, bounding_box_test, or query).\n",
        "            transform: Transformations to be applied on the images.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Loop over all jpg files in the directory\n",
        "        for file in os.listdir(root_dir):\n",
        "            if file.endswith('.jpg'):\n",
        "                # Extract the person ID from the file name.\n",
        "                # Files are assumed to have a format like \"0002_c1s1_000451_03.jpg\".\n",
        "                # However, files with junk/distractor images might have negative IDs like \"-1_c...\"\n",
        "                id_str = file.split('_')[0]  # Get the first part\n",
        "                # Skip if the id_str starts with '-' or is not composed of digits\n",
        "                if id_str.startswith('-') or not id_str.isdigit():\n",
        "                    continue\n",
        "                person_id = int(id_str)\n",
        "                # Optionally filter out junk or unwanted IDs (e.g., person_id <= 0)\n",
        "                if person_id <= 0:\n",
        "                    continue\n",
        "                self.image_paths.append(os.path.join(root_dir, file))\n",
        "                self.labels.append(person_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "iPvTyPPZoW3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Use the preprocessing pipeline defined earlier\n",
        "transform = preprocess_pipeline  # or define your own augmentations\n",
        "\n",
        "\n",
        "path = \"/kaggle/input/market-1501/Market-1501-v15.09.15\"\n",
        "# Build dataset paths\n",
        "train_dir = os.path.join(path, \"bounding_box_train\")\n",
        "test_dir = os.path.join(path, \"bounding_box_test\")  # For evaluation\n",
        "\n",
        "# Create dataset objects\n",
        "train_dataset = Market1501Dataset(root_dir=train_dir, transform=transform)\n",
        "test_dataset = Market1501Dataset(root_dir=test_dir, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of testing samples: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXLvWFRap9lV",
        "outputId": "41e3b298-f6de-47b2-c5a3-325882a7493f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 12936\n",
            "Number of testing samples: 13115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "# Preprocessing (ensure it suits both the ResNet backbone and CLIP; adjust if needed)\n",
        "preprocess_pipeline = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
        "        std=[0.229, 0.224, 0.225]    # ImageNet std\n",
        "    )\n",
        "])\n",
        "\n",
        "# Assume 'path' is the dataset root from kagglehub (e.g., downloaded from \"pengcw1/market-1501\")\n",
        "# Update the directory names as needed based on the actual dataset structure.\n",
        "train_dir = os.path.join(path, \"bounding_box_train\")\n",
        "# Build DataLoader for training set\n",
        "train_dataset = Market1501Dataset(root_dir=train_dir, transform=preprocess_pipeline)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Model Initialization (Fused Multimodal ReID Model)\n",
        "# ---------------------------\n",
        "# Using ReIDMultimodalNet as defined previously (which outputs a fused 256-D embedding)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = ReIDMultimodalNet(device=device).to(device)\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Training Loop Using Supervised Contrastive Loss\n",
        "# ---------------------------\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "num_epochs = 50  # You may adjust as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        embeddings = model(images)  # Fused embedding, shape (B, 256)\n",
        "\n",
        "        # Compute Supervised Contrastive Loss\n",
        "        loss = supervised_contrastive_loss(embeddings, labels, temperature=0.07)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Supervised Contrastive Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAmHw7xgqE_Y",
        "outputId": "557dcc2c-0a33-463d-b09b-5d2f5ba4ee91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 - Supervised Contrastive Loss: 0.1253\n",
            "Epoch 2/50 - Supervised Contrastive Loss: 0.1169\n",
            "Epoch 3/50 - Supervised Contrastive Loss: 0.1055\n",
            "Epoch 4/50 - Supervised Contrastive Loss: 0.0843\n",
            "Epoch 5/50 - Supervised Contrastive Loss: 0.0868\n",
            "Epoch 6/50 - Supervised Contrastive Loss: 0.0831\n",
            "Epoch 7/50 - Supervised Contrastive Loss: 0.0768\n",
            "Epoch 8/50 - Supervised Contrastive Loss: 0.0808\n",
            "Epoch 9/50 - Supervised Contrastive Loss: 0.0834\n",
            "Epoch 10/50 - Supervised Contrastive Loss: 0.0724\n",
            "Epoch 11/50 - Supervised Contrastive Loss: 0.0672\n",
            "Epoch 12/50 - Supervised Contrastive Loss: 0.0677\n",
            "Epoch 13/50 - Supervised Contrastive Loss: 0.0567\n",
            "Epoch 14/50 - Supervised Contrastive Loss: 0.0584\n",
            "Epoch 15/50 - Supervised Contrastive Loss: 0.0591\n",
            "Epoch 16/50 - Supervised Contrastive Loss: 0.0531\n",
            "Epoch 17/50 - Supervised Contrastive Loss: 0.0489\n",
            "Epoch 18/50 - Supervised Contrastive Loss: 0.0448\n",
            "Epoch 19/50 - Supervised Contrastive Loss: 0.0593\n",
            "Epoch 20/50 - Supervised Contrastive Loss: 0.0655\n",
            "Epoch 21/50 - Supervised Contrastive Loss: 0.0520\n",
            "Epoch 22/50 - Supervised Contrastive Loss: 0.0559\n",
            "Epoch 23/50 - Supervised Contrastive Loss: 0.0547\n",
            "Epoch 24/50 - Supervised Contrastive Loss: 0.0445\n",
            "Epoch 25/50 - Supervised Contrastive Loss: 0.0394\n",
            "Epoch 26/50 - Supervised Contrastive Loss: 0.0428\n",
            "Epoch 27/50 - Supervised Contrastive Loss: 0.0402\n",
            "Epoch 28/50 - Supervised Contrastive Loss: 0.0394\n",
            "Epoch 29/50 - Supervised Contrastive Loss: 0.0394\n",
            "Epoch 30/50 - Supervised Contrastive Loss: 0.0413\n",
            "Epoch 31/50 - Supervised Contrastive Loss: 0.0477\n",
            "Epoch 32/50 - Supervised Contrastive Loss: 0.0432\n",
            "Epoch 33/50 - Supervised Contrastive Loss: 0.0364\n",
            "Epoch 34/50 - Supervised Contrastive Loss: 0.0407\n",
            "Epoch 35/50 - Supervised Contrastive Loss: 0.0472\n",
            "Epoch 36/50 - Supervised Contrastive Loss: 0.0390\n",
            "Epoch 37/50 - Supervised Contrastive Loss: 0.0315\n",
            "Epoch 38/50 - Supervised Contrastive Loss: 0.0344\n",
            "Epoch 39/50 - Supervised Contrastive Loss: 0.0307\n",
            "Epoch 40/50 - Supervised Contrastive Loss: 0.0347\n",
            "Epoch 41/50 - Supervised Contrastive Loss: 0.0401\n",
            "Epoch 42/50 - Supervised Contrastive Loss: 0.0387\n",
            "Epoch 43/50 - Supervised Contrastive Loss: 0.0403\n",
            "Epoch 44/50 - Supervised Contrastive Loss: 0.0314\n",
            "Epoch 45/50 - Supervised Contrastive Loss: 0.0424\n",
            "Epoch 46/50 - Supervised Contrastive Loss: 0.0312\n",
            "Epoch 47/50 - Supervised Contrastive Loss: 0.0313\n",
            "Epoch 48/50 - Supervised Contrastive Loss: 0.0294\n",
            "Epoch 49/50 - Supervised Contrastive Loss: 0.0332\n",
            "Epoch 50/50 - Supervised Contrastive Loss: 0.0267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/reid_multimodal_model.pth\")\n"
      ],
      "metadata": {
        "id": "PgwG51j0rtxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dJrzawiexcMO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}