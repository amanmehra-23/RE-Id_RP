{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFA/Sj1/1XLUCXKgSuCFPU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanmehra-23/RE-Id_RP/blob/main/ReIdTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub openai-clip torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2Gg__V61erD",
        "outputId": "4a524bdb-99ad-4263-de6b-040759b3b909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.11)\n",
            "Requirement already satisfied: openai-clip in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from openai-clip) (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from openai-clip) (2024.11.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->openai-clip) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BT-LGd5y6hu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms,models\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "import requests\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# (Assumed) Definition of Market1501Dataset\n",
        "# ---------------------------\n",
        "class Market1501Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Directory with images (e.g., query, bounding_box_test).\n",
        "            transform: Transformations applied on images.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Loop over files in root_dir\n",
        "        for file in os.listdir(root_dir):\n",
        "            if file.endswith('.jpg'):\n",
        "                # Expected format: \"0002_c1s1_000451_03.jpg\"\n",
        "                id_str = file.split('_')[0]  # Get first token\n",
        "                # Skip distractors or junk (ids starting with '-' or non-digit)\n",
        "                if id_str.startswith('-') or not id_str.isdigit():\n",
        "                    continue\n",
        "                person_id = int(id_str)\n",
        "                if person_id <= 0:\n",
        "                    continue\n",
        "                self.image_paths.append(os.path.join(root_dir, file))\n",
        "                self.labels.append(person_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "SE7ec26pzKCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Define Preprocessing Pipeline\n",
        "# ---------------------------\n",
        "preprocess_pipeline = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
        "        std=[0.229, 0.224, 0.225]    # ImageNet std\n",
        "    )\n",
        "])"
      ],
      "metadata": {
        "id": "587vBVKHz7FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"pengcw1/market-1501\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "4Dg9-pat0XaX",
        "outputId": "9269424a-5e88-4929-fa60-587dfae9f34d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/market-1501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Set Dataset Paths\n",
        "# ---------------------------\n",
        "# 'path' should be set to the root directory you downloaded via KaggleHub.\n",
        "# Adjust these as necessary based on your folder structure.\n",
        "dataset_path = \"/kaggle/input/market-1501/Market-1501-v15.09.15\"  # Change as needed\n",
        "query_dir = os.path.join(dataset_path, \"query\")\n",
        "gallery_dir = os.path.join(dataset_path, \"bounding_box_test\")\n"
      ],
      "metadata": {
        "id": "BKwNHYbLz-LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Create Dataset and DataLoader Objects\n",
        "# ---------------------------\n",
        "query_dataset = Market1501Dataset(root_dir=query_dir, transform=preprocess_pipeline)\n",
        "gallery_dataset = Market1501Dataset(root_dir=gallery_dir, transform=preprocess_pipeline)\n",
        "\n",
        "# It is often useful to use a lower number of workers to avoid freezing (e.g., num_workers=2)\n",
        "query_loader = DataLoader(query_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "gallery_loader = DataLoader(gallery_dataset, batch_size=32, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "6FyT7uMq0AZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetBackbone(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(ResNetBackbone, self).__init__()\n",
        "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "        # Remove the final pooling and FC layers: output shape (B, 2048, 7, 7)\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)  # Expected shape: (B, 2048, 7, 7)"
      ],
      "metadata": {
        "id": "ZZJi2bAK4UU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 2: Build Grid Graph ---\n",
        "def build_grid_edge_index(grid_size):\n",
        "    \"\"\"\n",
        "    Constructs edge indices for a grid graph given grid dimensions.\n",
        "    Each node (patch) is connected to its right and down neighbor (and vice versa).\n",
        "    \"\"\"\n",
        "    H, W = grid_size\n",
        "    edges = []\n",
        "    for i in range(H):\n",
        "        for j in range(W):\n",
        "            idx = i * W + j\n",
        "            # Connect to right neighbor if exists\n",
        "            if j + 1 < W:\n",
        "                right_idx = i * W + (j + 1)\n",
        "                edges.append((idx, right_idx))\n",
        "                edges.append((right_idx, idx))\n",
        "            # Connect to down neighbor if exists\n",
        "            if i + 1 < H:\n",
        "                down_idx = (i + 1) * W + j\n",
        "                edges.append((idx, down_idx))\n",
        "                edges.append((down_idx, idx))\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    return edge_index  # Shape: (2, num_edges)"
      ],
      "metadata": {
        "id": "cyg90E_X4ef-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 3: GNN Branch ---\n",
        "class GNNBranch(nn.Module):\n",
        "    def __init__(self, in_channels=2048, hidden_channels=512, out_channels=256, grid_size=(7,7)):\n",
        "        super(GNNBranch, self).__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.edge_index = build_grid_edge_index(grid_size)  # Fixed for a given grid size\n",
        "\n",
        "        # Two GCN layers\n",
        "        self.gcn1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.gcn2 = GCNConv(hidden_channels, out_channels)\n",
        "        # Optional FC layer for further refinement\n",
        "        self.fc = nn.Linear(out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: CNN feature map of shape (B, 2048, H, W) with H,W = grid_size (e.g., 7,7)\n",
        "        Returns:\n",
        "            A tensor of shape (B, out_channels) representing the person embedding.\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        N = H * W  # Number of nodes (e.g., 49)\n",
        "        # Reshape: (B, C, H, W) -> (B, N, C)\n",
        "        x_nodes = x.view(B, C, N).permute(0, 2, 1)  # (B, N, 2048)\n",
        "        embeddings = []\n",
        "        edge_index = self.edge_index.to(x.device)  # Ensure edge_index is on the same device\n",
        "        for i in range(B):\n",
        "            node_feat = x_nodes[i]  # (N, 2048)\n",
        "            h = F.relu(self.gcn1(node_feat, edge_index))  # (N, hidden_channels)\n",
        "            h = self.gcn2(h, edge_index)  # (N, out_channels)\n",
        "            # Global mean pooling: average over the N nodes\n",
        "            pooled = h.mean(dim=0)  # (out_channels,)\n",
        "            embeddings.append(pooled)\n",
        "        embeddings = torch.stack(embeddings, dim=0)  # (B, out_channels)\n",
        "        embeddings = self.fc(embeddings)\n",
        "        return embeddings  # (B, out_channels) e.g., (B, 256)\n"
      ],
      "metadata": {
        "id": "RSU_Pfff4aVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 4: CLIP Branch ---\n",
        "# For the CLIP branch, we use OpenAI's CLIP model.\n",
        "# Ensure you have installed the clip package (e.g., pip install git+https://github.com/openai/CLIP.git)\n",
        "import clip\n",
        "\n",
        "class CLIPBranch(nn.Module):\n",
        "    def __init__(self, device='cuda'):\n",
        "        super(CLIPBranch, self).__init__()\n",
        "        self.clip_model, self.clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "        self.clip_model.eval()  # Set to eval mode\n",
        "        self.proj = nn.Linear(512, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure x is on the correct device\n",
        "        x = x.to(next(self.clip_model.parameters()).device)\n",
        "        with torch.no_grad():\n",
        "            clip_emb = self.clip_model.encode_image(x)  # (B, 512)\n",
        "        # Convert to float32 to match the projection layer parameters\n",
        "        clip_emb = clip_emb.float()\n",
        "        clip_emb = self.proj(clip_emb)  # (B, 256)\n",
        "        return clip_emb"
      ],
      "metadata": {
        "id": "T5LLGSHI4kdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 5: Fusion Module ---\n",
        "class FusionModule(nn.Module):\n",
        "    def __init__(self, emb_dim=256):\n",
        "        super(FusionModule, self).__init__()\n",
        "        # Fusion via concatenation then projection to emb_dim\n",
        "        self.fc = nn.Linear(emb_dim * 2, emb_dim)\n",
        "\n",
        "    def forward(self, gnn_emb, clip_emb):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            gnn_emb: Embedding from GNN branch (B, emb_dim)\n",
        "            clip_emb: Embedding from CLIP branch (B, emb_dim)\n",
        "        Returns:\n",
        "            Fused embedding (B, emb_dim)\n",
        "        \"\"\"\n",
        "        fused = torch.cat([gnn_emb, clip_emb], dim=1)  # (B, 2*emb_dim)\n",
        "        fused = self.fc(fused)\n",
        "        return fused"
      ],
      "metadata": {
        "id": "P-5GsM484uLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReIDMultimodalNet(nn.Module):\n",
        "    def __init__(self, device='cuda'):\n",
        "        super(ReIDMultimodalNet, self).__init__()\n",
        "        self.device = device\n",
        "        self.backbone = ResNetBackbone(pretrained=True)\n",
        "        self.gnn_branch = GNNBranch(in_channels=2048, hidden_channels=512, out_channels=256, grid_size=(7,7))\n",
        "        self.clip_branch = CLIPBranch(device=device)\n",
        "        self.fusion = FusionModule(emb_dim=256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input image tensor of shape (B, 3, 224, 224)\n",
        "        Returns:\n",
        "            Fused multimodal embedding (B, 256)\n",
        "        \"\"\"\n",
        "        x = x.to(self.device)\n",
        "        # CNN backbone to get feature map: (B, 2048, 7, 7)\n",
        "        feature_map = self.backbone(x)\n",
        "        # GNN branch: process feature map and produce a 256-D embedding\n",
        "        gnn_emb = self.gnn_branch(feature_map)\n",
        "        # CLIP branch: process the image and produce a 256-D embedding\n",
        "        clip_emb = self.clip_branch(x)\n",
        "        # Fusion: combine the two embeddings\n",
        "        fused_emb = self.fusion(gnn_emb, clip_emb)\n",
        "        return fused_emb"
      ],
      "metadata": {
        "id": "XykJhVkJ3Aqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------------------------------\n",
        "# 5. Define Helper Functions for Evaluation\n",
        "# -------------------------------------------------\n",
        "def extract_embeddings(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Extract embeddings and labels for all images in a DataLoader.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_embeddings = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            embeddings = model(images)  # Expect shape: (B, 256)\n",
        "            all_embeddings.append(embeddings.cpu())\n",
        "            all_labels.extend(labels.numpy())\n",
        "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "    return all_embeddings, np.array(all_labels)\n",
        "\n",
        "def compute_distance_matrix(query_emb, gallery_emb, metric='euclidean'):\n",
        "    \"\"\"\n",
        "    Compute a pairwise distance matrix between query and gallery embeddings.\n",
        "\n",
        "    Args:\n",
        "        query_emb: Tensor (num_query, D)\n",
        "        gallery_emb: Tensor (num_gallery, D)\n",
        "        metric: 'euclidean' or 'cosine'\n",
        "    Returns:\n",
        "        dist_matrix: Tensor (num_query, num_gallery)\n",
        "    \"\"\"\n",
        "    if metric == 'euclidean':\n",
        "        dist_matrix = torch.cdist(query_emb, gallery_emb, p=2)\n",
        "    elif metric == 'cosine':\n",
        "        query_norm = F.normalize(query_emb, p=2, dim=1)\n",
        "        gallery_norm = F.normalize(gallery_emb, p=2, dim=1)\n",
        "        dist_matrix = 1 - torch.mm(query_norm, gallery_norm.t())\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported metric\")\n",
        "    return dist_matrix\n",
        "\n",
        "def evaluate_rank1_map(dist_matrix, query_labels, gallery_labels):\n",
        "    \"\"\"\n",
        "    Compute Rank-1 accuracy and mean Average Precision (mAP) given a distance matrix.\n",
        "    \"\"\"\n",
        "    num_queries = dist_matrix.size(0)\n",
        "    rank1 = 0\n",
        "    ap_list = []\n",
        "\n",
        "    query_labels = np.array(query_labels)\n",
        "    gallery_labels = np.array(gallery_labels)\n",
        "\n",
        "    for i in range(num_queries):\n",
        "        distances = dist_matrix[i].cpu().numpy()\n",
        "        sorted_indices = np.argsort(distances)\n",
        "        matches = (gallery_labels[sorted_indices] == query_labels[i])\n",
        "\n",
        "        if matches[0]:\n",
        "            rank1 += 1\n",
        "\n",
        "        num_relevant = matches.sum()\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "\n",
        "        precisions = []\n",
        "        correct = 0\n",
        "        for j, flag in enumerate(matches):\n",
        "            if flag:\n",
        "                correct += 1\n",
        "                precisions.append(correct / (j + 1))\n",
        "        ap_list.append(np.mean(precisions))\n",
        "\n",
        "    rank1_accuracy = rank1 / num_queries\n",
        "    mAP = np.mean(ap_list) if ap_list else 0\n",
        "    return rank1_accuracy, mAP\n",
        "\n"
      ],
      "metadata": {
        "id": "ynzQor6F5VD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/reid_multimodal_model.pth\", map_location=device))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABdr94oG5-bQ",
        "outputId": "31076461-875d-404a-c2c2-f1b4f600bb4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ReIDMultimodalNet(\n",
              "  (backbone): ResNetBackbone(\n",
              "    (features): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (5): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (6): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (7): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (gnn_branch): GNNBranch(\n",
              "    (gcn1): GCNConv(2048, 512)\n",
              "    (gcn2): GCNConv(512, 256)\n",
              "    (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "  )\n",
              "  (clip_branch): CLIPBranch(\n",
              "    (clip_model): CLIP(\n",
              "      (visual): VisionTransformer(\n",
              "        (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (transformer): Transformer(\n",
              "          (resblocks): Sequential(\n",
              "            (0): ResidualAttentionBlock(\n",
              "              (attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Sequential(\n",
              "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (gelu): QuickGELU()\n",
              "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "            (1): ResidualAttentionBlock(\n",
              "              (attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Sequential(\n",
              "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (gelu): QuickGELU()\n",
              "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "            (2): ResidualAttentionBlock(\n",
              "              (attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Sequential(\n",
              "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (gelu): QuickGELU()\n",
              "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "            (3): ResidualAttentionBlock(\n",
              "              (attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Sequential(\n",
              "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (gelu): QuickGELU()\n",
              "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "            (4): ResidualAttentionBlock(\n",
              "              (attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Sequential(\n",
              "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (gelu): QuickGELU()\n",
              "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "            (5): ResidualAttentionBlock(\n",
              "              (attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Sequential(\n",
              "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (gelu): QuickGELU()\n",
              "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "            (6): ResidualAttentionBlock(\n",
              "              (attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Sequential(\n",
              "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (gelu): QuickGELU()\n",
              "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "            (7): ResidualAttentionBlock(\n",
              "              (attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Sequential(\n",
              "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (gelu): QuickGELU()\n",
              "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "            (8): ResidualAttentionBlock(\n",
              "              (attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Sequential(\n",
              "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (gelu): QuickGELU()\n",
              "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "            (9): ResidualAttentionBlock(\n",
              "              (attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Sequential(\n",
              "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (gelu): QuickGELU()\n",
              "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "            (10): ResidualAttentionBlock(\n",
              "              (attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Sequential(\n",
              "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (gelu): QuickGELU()\n",
              "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "            (11): ResidualAttentionBlock(\n",
              "              (attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Sequential(\n",
              "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (gelu): QuickGELU()\n",
              "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              )\n",
              "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (transformer): Transformer(\n",
              "        (resblocks): Sequential(\n",
              "          (0): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (6): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (7): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (8): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (9): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (10): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (11): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (token_embedding): Embedding(49408, 512)\n",
              "      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (proj): Linear(in_features=512, out_features=256, bias=True)\n",
              "  )\n",
              "  (fusion): FusionModule(\n",
              "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------\n",
        "# 6. Run the Evaluation\n",
        "# -------------------------------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Assuming your model is already loaded and set to eval mode\n",
        "# (Your model instance 'model' is loaded with the saved checkpoint as shown before)\n",
        "query_embeddings, query_labels = extract_embeddings(model, query_loader, device)\n",
        "gallery_embeddings, gallery_labels = extract_embeddings(model, gallery_loader, device)\n",
        "\n",
        "# Choose a metric: 'euclidean' or 'cosine'\n",
        "dist_matrix = compute_distance_matrix(query_embeddings, gallery_embeddings, metric='cosine')\n",
        "\n",
        "rank1_accuracy, mAP = evaluate_rank1_map(dist_matrix, query_labels, gallery_labels)\n",
        "print(\"Rank-1 Accuracy: {:.2%}\".format(rank1_accuracy))\n",
        "print(\"mAP: {:.2%}\".format(mAP))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxrLvW225yzm",
        "outputId": "3db6ffe4-2e7b-48a0-962f-676a0637865b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank-1 Accuracy: 86.88%\n",
            "mAP: 37.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R4LtXpwU5za_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}