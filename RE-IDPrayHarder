{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9b6d326",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-15T20:10:30.820621Z",
     "iopub.status.busy": "2025-04-15T20:10:30.820414Z",
     "iopub.status.idle": "2025-04-15T20:12:18.717899Z",
     "shell.execute_reply": "2025-04-15T20:12:18.717048Z"
    },
    "papermill": {
     "duration": 107.903236,
     "end_time": "2025-04-15T20:12:18.719762",
     "exception": false,
     "start_time": "2025-04-15T20:10:30.816526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "          os.path.join(dirname, filename)\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f71e802c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T20:12:18.730639Z",
     "iopub.status.busy": "2025-04-15T20:12:18.730299Z",
     "iopub.status.idle": "2025-04-15T20:12:25.378492Z",
     "shell.execute_reply": "2025-04-15T20:12:25.377570Z"
    },
    "papermill": {
     "duration": 6.654685,
     "end_time": "2025-04-15T20:12:25.379924",
     "exception": false,
     "start_time": "2025-04-15T20:12:18.725239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai-clip\r\n",
      "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting ftfy (from openai-clip)\r\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\r\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from openai-clip) (2024.11.6)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-clip) (4.67.1)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->openai-clip) (0.2.13)\r\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: openai-clip\r\n",
      "  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=2dd0a902192ae3a4b778743ac8f1018708000e5f23f06d63827d5f103d6c7a29\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/17/90/042948fd2e2a87f1dcf6db6d438cad015c49db0c53d1d9c7dc\r\n",
      "Successfully built openai-clip\r\n",
      "Installing collected packages: ftfy, openai-clip\r\n",
      "Successfully installed ftfy-6.3.1 openai-clip-1.0.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install openai-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2f0c3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T20:12:25.387593Z",
     "iopub.status.busy": "2025-04-15T20:12:25.387352Z",
     "iopub.status.idle": "2025-04-15T20:13:03.298066Z",
     "shell.execute_reply": "2025-04-15T20:13:03.297407Z"
    },
    "papermill": {
     "duration": 37.915871,
     "end_time": "2025-04-15T20:13:03.299432",
     "exception": false,
     "start_time": "2025-04-15T20:12:25.383561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [00:18<00:00, 19.3MiB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "# Import and load CLIP along with its preprocessing.\n",
    "import clip\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4013492a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T20:13:03.314266Z",
     "iopub.status.busy": "2025-04-15T20:13:03.313692Z",
     "iopub.status.idle": "2025-04-15T20:13:03.320788Z",
     "shell.execute_reply": "2025-04-15T20:13:03.320182Z"
    },
    "papermill": {
     "duration": 0.015613,
     "end_time": "2025-04-15T20:13:03.322052",
     "exception": false,
     "start_time": "2025-04-15T20:13:03.306439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 2. Modified Dataset: Return Two Preprocessed Versions\n",
    "#############################################\n",
    "class Market1501Dataset(Dataset):\n",
    "    def __init__(self, root, transform_eff=None, transform_clip=None):\n",
    "        self.root = root\n",
    "        # List only jpg images.\n",
    "        self.image_paths = [os.path.join(root, fname) for fname in os.listdir(root) if fname.endswith('.jpg')]\n",
    "        # Extract raw labels.\n",
    "        self.labels = [self._extract_label(fname) for fname in os.listdir(root) if fname.endswith('.jpg')]\n",
    "        # Build mapping from raw label to index.\n",
    "        self.unique_labels = sorted(set(self.labels))\n",
    "        self.label2index = {label: idx for idx, label in enumerate(self.unique_labels)}\n",
    "        self.labels = [self.label2index[label] for label in self.labels]\n",
    "        self.transform_eff = transform_eff\n",
    "        self.transform_clip = transform_clip\n",
    "\n",
    "    def _extract_label(self, filename):\n",
    "        pid = filename.split('_')[0]\n",
    "        if pid == '-1':\n",
    "            return -1\n",
    "        return int(pid)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.image_paths[index]\n",
    "        label = self.labels[index]\n",
    "        # Open image in PIL format.\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        # Apply EfficientNet branch transform.\n",
    "        if self.transform_eff:\n",
    "            img_eff = self.transform_eff(image)\n",
    "        else:\n",
    "            img_eff = image\n",
    "        # Apply CLIP's preprocessing (which produces a 224x224 image as expected).\n",
    "        if self.transform_clip:\n",
    "            img_clip = self.transform_clip(image)\n",
    "        else:\n",
    "            img_clip = image\n",
    "        # Return a tuple ((efficient_net_image, clip_image), label)\n",
    "        return (img_eff, img_clip), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "070beb0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T20:13:03.335834Z",
     "iopub.status.busy": "2025-04-15T20:13:03.335659Z",
     "iopub.status.idle": "2025-04-15T20:13:03.340369Z",
     "shell.execute_reply": "2025-04-15T20:13:03.339697Z"
    },
    "papermill": {
     "duration": 0.012679,
     "end_time": "2025-04-15T20:13:03.341376",
     "exception": false,
     "start_time": "2025-04-15T20:13:03.328697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 3. Define Separate Transforms for Each Branch\n",
    "#############################################\n",
    "# For the EfficientNet branch, we keep your custom resizing and augmentation.\n",
    "train_transforms_eff = transforms.Compose([\n",
    "    transforms.Resize((256, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms_eff = transforms.Compose([\n",
    "    transforms.Resize((256, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# For the CLIP branch, we use its provided preprocessing.\n",
    "# clip_preprocess is already a torchvision transform that resizes the image to 224x224,\n",
    "# center crops, converts to tensor, and normalizes using CLIP's expected values.\n",
    "train_transforms_clip = clip_preprocess\n",
    "test_transforms_clip = clip_preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9992acd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T20:13:03.355441Z",
     "iopub.status.busy": "2025-04-15T20:13:03.354761Z",
     "iopub.status.idle": "2025-04-15T20:13:03.363592Z",
     "shell.execute_reply": "2025-04-15T20:13:03.363136Z"
    },
    "papermill": {
     "duration": 0.016758,
     "end_time": "2025-04-15T20:13:03.364511",
     "exception": false,
     "start_time": "2025-04-15T20:13:03.347753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 4. Model Architecture\n",
    "#############################################\n",
    "# Squeeze-and-Excitation (SE) block.\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        y = x.view(b, c, -1).mean(dim=2)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "# Original EfficientNet-based re-ID branch.\n",
    "class EfficientNetReID(nn.Module):\n",
    "    def __init__(self, num_classes, backbone='tf_efficientnet_b0_ns', pretrained=True, parts=3):\n",
    "        super(EfficientNetReID, self).__init__()\n",
    "        self.parts = parts\n",
    "        self.backbone = timm.create_model(backbone, pretrained=pretrained, features_only=True)\n",
    "        self.feature_dim = self.backbone.feature_info[-1]['num_chs']\n",
    "        self.se = SEBlock(self.feature_dim)\n",
    "        self.part_fc = nn.ModuleList([nn.Linear(self.feature_dim, num_classes) for _ in range(parts)])\n",
    "        self.part_embedding = nn.ModuleList([nn.Linear(self.feature_dim, 256) for _ in range(parts)])\n",
    "        self.classifier = nn.Linear(256 * parts, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)[-1]  # shape: [batch, channels, height, width]\n",
    "        feats = self.se(feats)\n",
    "        b, c, h, w = feats.size()\n",
    "        part_h = h // self.parts\n",
    "\n",
    "        global_features = []\n",
    "        part_logits = []\n",
    "        for i in range(self.parts):\n",
    "            if i == self.parts - 1:\n",
    "                part = feats[:, :, i * part_h:, :]\n",
    "            else:\n",
    "                part = feats[:, :, i * part_h:(i + 1) * part_h, :]\n",
    "            part_pool = part.mean(dim=[2, 3])\n",
    "            part_embed = torch.relu(self.part_embedding[i](part_pool))\n",
    "            global_features.append(part_embed)\n",
    "            part_logits.append(self.part_fc[i](part_pool))\n",
    "        global_feature = torch.cat(global_features, dim=1)\n",
    "        global_feature = nn.functional.normalize(global_feature, p=2, dim=1)\n",
    "        return global_feature, part_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec5f947",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T20:13:03.378221Z",
     "iopub.status.busy": "2025-04-15T20:13:03.378019Z",
     "iopub.status.idle": "2025-04-15T20:13:03.389278Z",
     "shell.execute_reply": "2025-04-15T20:13:03.388624Z"
    },
    "papermill": {
     "duration": 0.019414,
     "end_time": "2025-04-15T20:13:03.390325",
     "exception": false,
     "start_time": "2025-04-15T20:13:03.370911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#############################################\n",
    "# 5. Loss Functions and Training Helpers\n",
    "#############################################\n",
    "def pairwise_distance(embeddings, squared=False):\n",
    "    dot_product = torch.matmul(embeddings, embeddings.t())\n",
    "    square_norm = torch.diag(dot_product)\n",
    "    distances = square_norm.unsqueeze(1) - 2 * dot_product + square_norm.unsqueeze(0)\n",
    "    distances = torch.clamp(distances, min=0.0)\n",
    "    if not squared:\n",
    "        distances = torch.sqrt(distances + 1e-16)\n",
    "    return distances\n",
    "\n",
    "class BatchHardTripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.3):\n",
    "        super(BatchHardTripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n",
    "\n",
    "    def forward(self, embeddings, labels):\n",
    "        pdist = pairwise_distance(embeddings)\n",
    "        n = embeddings.size(0)\n",
    "        mask_positive = labels.expand(n, n).eq(labels.expand(n, n).t())\n",
    "        mask_negative = ~mask_positive\n",
    "\n",
    "        hardest_positive = []\n",
    "        hardest_negative = []\n",
    "        for i in range(n):\n",
    "            pos_distances = pdist[i][mask_positive[i]]\n",
    "            neg_distances = pdist[i][mask_negative[i]]\n",
    "            hardest_positive.append(pos_distances.max())\n",
    "            hardest_negative.append(neg_distances.min())\n",
    "        hardest_positive = torch.stack(hardest_positive)\n",
    "        hardest_negative = torch.stack(hardest_negative)\n",
    "        target = torch.ones_like(hardest_positive)\n",
    "        loss = self.ranking_loss(hardest_negative, hardest_positive, target)\n",
    "        return loss\n",
    "\n",
    "def train_multi(model, train_loader, optimizer, criterion_ce, criterion_tri, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for (img_eff, img_clip), labels in train_loader:\n",
    "        img_eff = img_eff.to(device)\n",
    "        img_clip = img_clip.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(img_eff, img_clip)\n",
    "        loss_tri_eff = criterion_tri(outputs[\"efficient_feature\"], labels)\n",
    "        loss_tri_clip = criterion_tri(outputs[\"clip_feature\"], labels)\n",
    "        loss_ce_eff = criterion_ce(outputs[\"efficient_logits\"], labels)\n",
    "        loss_ce_clip = criterion_ce(outputs[\"clip_logits\"], labels)\n",
    "        loss_ce_fused = criterion_ce(outputs[\"fused_logits\"], labels)\n",
    "        loss = loss_tri_eff + loss_tri_clip + loss_ce_eff + loss_ce_clip + loss_ce_fused\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate(model, val_loader, criterion_ce, criterion_tri, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (img_eff, img_clip), labels in val_loader:\n",
    "            img_eff = img_eff.to(device)\n",
    "            img_clip = img_clip.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(img_eff, img_clip)\n",
    "            loss_tri_eff = criterion_tri(outputs[\"efficient_feature\"], labels)\n",
    "            loss_tri_clip = criterion_tri(outputs[\"clip_feature\"], labels)\n",
    "            loss_ce_eff = criterion_ce(outputs[\"efficient_logits\"], labels)\n",
    "            loss_ce_clip = criterion_ce(outputs[\"clip_logits\"], labels)\n",
    "            loss_ce_fused = criterion_ce(outputs[\"fused_logits\"], labels)\n",
    "            loss = loss_tri_eff + loss_tri_clip + loss_ce_eff + loss_ce_clip + loss_ce_fused\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "def extract_features_multi(model, dataloader, device):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels_all = []\n",
    "    with torch.no_grad():\n",
    "        for (img_eff, img_clip), labels in dataloader:\n",
    "            img_eff = img_eff.to(device)\n",
    "            img_clip = img_clip.to(device)\n",
    "            outputs = model(img_eff, img_clip)\n",
    "            feat = outputs[\"fused_feature\"]\n",
    "            features.append(feat.cpu())\n",
    "            labels_all.append(labels)\n",
    "    features = torch.cat(features, dim=0)\n",
    "    labels_all = torch.cat(labels_all, dim=0)\n",
    "    return features.numpy(), labels_all.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50ec0ecc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T20:13:03.403863Z",
     "iopub.status.busy": "2025-04-15T20:13:03.403632Z",
     "iopub.status.idle": "2025-04-15T20:13:03.410144Z",
     "shell.execute_reply": "2025-04-15T20:13:03.409615Z"
    },
    "papermill": {
     "duration": 0.014396,
     "end_time": "2025-04-15T20:13:03.411104",
     "exception": false,
     "start_time": "2025-04-15T20:13:03.396708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The multipath ReID model that integrates the EfficientNet branch with a CLIP branch.\n",
    "class MultiModelReID(nn.Module):\n",
    "    def __init__(self, num_classes, parts=3, efficient_backbone='tf_efficientnet_b0_ns',\n",
    "                 efficient_pretrained=True, clip_model_name=\"ViT-B/32\", device=torch.device(\"cpu\")):\n",
    "        super(MultiModelReID, self).__init__()\n",
    "        self.parts = parts\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # EfficientNet branch\n",
    "        self.efficient_net = EfficientNetReID(num_classes=num_classes,\n",
    "                                              backbone=efficient_backbone,\n",
    "                                              pretrained=efficient_pretrained,\n",
    "                                              parts=parts)\n",
    "        # Use the already-loaded CLIP model and its preprocess.\n",
    "        self.clip_model = clip_model  # from earlier\n",
    "        # Freeze CLIP parameters if desired.\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Projection to match dimensions: projecting CLIP's 512 dims to 256*parts.\n",
    "        self.clip_proj = nn.Linear(512, 256 * parts)\n",
    "        self.clip_classifier = nn.Linear(256 * parts, num_classes)\n",
    "        self.fusion_classifier = nn.Linear(256 * parts, num_classes)\n",
    "\n",
    "    # Now the forward accepts two inputs: one for each branch.\n",
    "    def forward(self, img_eff, img_clip):\n",
    "        # EfficientNet branch forward.\n",
    "        efficient_feature, efficient_logits_list = self.efficient_net(img_eff)\n",
    "        efficient_logits = sum(efficient_logits_list) / len(efficient_logits_list)\n",
    "        \n",
    "        # CLIP branch forward.\n",
    "        clip_features = self.clip_model.encode_image(img_clip)  # returns features in half precision\n",
    "        clip_features = clip_features.float()  # Convert to float32 to match the projection layer\n",
    "        clip_features = self.clip_proj(clip_features)\n",
    "        clip_features = torch.relu(clip_features)\n",
    "        clip_features = nn.functional.normalize(clip_features, p=2, dim=1)\n",
    "        # Compute CLIP logits using the dedicated classifier\n",
    "        clip_logits = self.clip_classifier(clip_features)\n",
    "        \n",
    "        # Fuse features from both branches (here by averaging).\n",
    "        fused_feature = (efficient_feature + clip_features) / 2\n",
    "        fused_feature = nn.functional.normalize(fused_feature, p=2, dim=1)\n",
    "        fused_logits = self.fusion_classifier(fused_feature)\n",
    "        \n",
    "        return {\n",
    "            \"efficient_feature\": efficient_feature,\n",
    "            \"efficient_logits\": efficient_logits,\n",
    "            \"clip_feature\": clip_features,\n",
    "            \"clip_logits\": clip_logits,\n",
    "            \"fused_feature\": fused_feature,\n",
    "            \"fused_logits\": fused_logits\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eca0971",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T20:13:03.424912Z",
     "iopub.status.busy": "2025-04-15T20:13:03.424707Z",
     "iopub.status.idle": "2025-04-15T20:13:03.432331Z",
     "shell.execute_reply": "2025-04-15T20:13:03.431780Z"
    },
    "papermill": {
     "duration": 0.015739,
     "end_time": "2025-04-15T20:13:03.433319",
     "exception": false,
     "start_time": "2025-04-15T20:13:03.417580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 5. Training, Validation, and Feature Extraction\n",
    "#############################################\n",
    "\n",
    "def train_multi(model, train_loader, optimizer, criterion_ce, criterion_tri, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for (img_eff, img_clip), labels in train_loader:\n",
    "        img_eff = img_eff.to(device)\n",
    "        img_clip = img_clip.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(img_eff, img_clip)\n",
    "        loss_tri_eff = criterion_tri(outputs[\"efficient_feature\"], labels)\n",
    "        loss_tri_clip = criterion_tri(outputs[\"clip_feature\"], labels)\n",
    "        loss_ce_eff = criterion_ce(outputs[\"efficient_logits\"], labels)\n",
    "        loss_ce_clip = criterion_ce(outputs[\"clip_logits\"], labels)\n",
    "        loss_ce_fused = criterion_ce(outputs[\"fused_logits\"], labels)\n",
    "        loss = loss_tri_eff + loss_tri_clip + loss_ce_eff + loss_ce_clip + loss_ce_fused\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion_ce, criterion_tri, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        # Unpack each batch into (img_eff, img_clip) and labels.\n",
    "        for (img_eff, img_clip), labels in val_loader:\n",
    "            img_eff = img_eff.to(device)\n",
    "            img_clip = img_clip.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(img_eff, img_clip)\n",
    "            loss_tri_eff = criterion_tri(outputs[\"efficient_feature\"], labels)\n",
    "            loss_tri_clip = criterion_tri(outputs[\"clip_feature\"], labels)\n",
    "            loss_ce_eff = criterion_ce(outputs[\"efficient_logits\"], labels)\n",
    "            loss_ce_clip = criterion_ce(outputs[\"clip_logits\"], labels)\n",
    "            loss_ce_fused = criterion_ce(outputs[\"fused_logits\"], labels)\n",
    "            loss = loss_tri_eff + loss_tri_clip + loss_ce_eff + loss_ce_clip + loss_ce_fused\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "\n",
    "def extract_features_multi(model, dataloader, device):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels_all = []\n",
    "    with torch.no_grad():\n",
    "        # Unpack the two sets of images.\n",
    "        for (img_eff, img_clip), labels in dataloader:\n",
    "            img_eff = img_eff.to(device)\n",
    "            img_clip = img_clip.to(device)\n",
    "            outputs = model(img_eff, img_clip)\n",
    "            feat = outputs[\"fused_feature\"]\n",
    "            features.append(feat.cpu())\n",
    "            labels_all.append(labels)\n",
    "    features = torch.cat(features, dim=0)\n",
    "    labels_all = torch.cat(labels_all, dim=0)\n",
    "    return features.numpy(), labels_all.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2216f50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T20:13:03.447359Z",
     "iopub.status.busy": "2025-04-15T20:13:03.447184Z",
     "iopub.status.idle": "2025-04-15T20:13:03.458004Z",
     "shell.execute_reply": "2025-04-15T20:13:03.457333Z"
    },
    "papermill": {
     "duration": 0.019403,
     "end_time": "2025-04-15T20:13:03.459211",
     "exception": false,
     "start_time": "2025-04-15T20:13:03.439808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 6. Re-Ranking and Evaluation Functions (unchanged)\n",
    "#############################################\n",
    "def re_ranking(query_features, gallery_features, k1=20, k2=6, lambda_value=0.3):\n",
    "    all_features = np.concatenate([query_features, gallery_features], axis=0)\n",
    "    squared = np.sum(np.power(all_features, 2), axis=1, keepdims=True)\n",
    "    original_dist = squared + squared.T - 2 * np.dot(all_features, all_features.T)\n",
    "    original_dist = np.sqrt(np.maximum(original_dist, 0))\n",
    "    original_dist = np.transpose(original_dist / np.max(original_dist, axis=0))\n",
    "\n",
    "    all_num = original_dist.shape[0]\n",
    "    V = np.zeros_like(original_dist).astype(np.float32)\n",
    "    initial_rank = np.argsort(original_dist, axis=1).astype(np.int32)\n",
    "\n",
    "    for i in range(all_num):\n",
    "        forward_neighbors = initial_rank[i, :k1 + 1]\n",
    "        backward_neighbors = initial_rank[forward_neighbors, :k1 + 1]\n",
    "        fi = np.where(backward_neighbors == i)[0]\n",
    "        k_reciprocal_index = forward_neighbors[fi]\n",
    "        k_reciprocal_expansion = k_reciprocal_index.copy()\n",
    "        for candidate in k_reciprocal_index:\n",
    "            candidate_forward = initial_rank[candidate, :int(np.around(k1 / 2)) + 1]\n",
    "            candidate_backward = initial_rank[candidate_forward, :int(np.around(k1 / 2)) + 1]\n",
    "            fi_candidate = np.where(candidate_backward == candidate)[0]\n",
    "            candidate_rec = candidate_forward[fi_candidate]\n",
    "            if len(np.intersect1d(candidate_rec, k_reciprocal_index)) > 2 / 3 * len(candidate_rec):\n",
    "                k_reciprocal_expansion = np.union1d(k_reciprocal_expansion, candidate_rec)\n",
    "        weight = np.exp(-original_dist[i, k_reciprocal_expansion])\n",
    "        V[i, k_reciprocal_expansion] = weight / np.sum(weight)\n",
    "    V = V.astype(np.float32)\n",
    "    invIndex = []\n",
    "    for i in range(all_num):\n",
    "        invIndex.append(np.where(V[:, i] != 0)[0])\n",
    "    jaccard_dist = np.zeros_like(original_dist, dtype=np.float32)\n",
    "    for i in range(all_num):\n",
    "        temp_min = np.zeros((1, all_num), dtype=np.float32)\n",
    "        non_zero_ind = np.where(V[i, :] != 0)[0]\n",
    "        for j in non_zero_ind:\n",
    "            temp_min[0, invIndex[j]] += np.minimum(V[i, j], V[invIndex[j], j])\n",
    "        jaccard_dist[i] = 1 - temp_min / (2 - temp_min)\n",
    "    final_dist = (1 - lambda_value) * jaccard_dist + lambda_value * original_dist\n",
    "    num_query = query_features.shape[0]\n",
    "    return final_dist[:num_query, num_query:]\n",
    "\n",
    "def evaluate_rank(query_features, query_labels, gallery_features, gallery_labels, max_rank=50):\n",
    "    num_q, num_g = query_features.shape[0], gallery_features.shape[0]\n",
    "    distmat = np.linalg.norm(query_features[:, np.newaxis] - gallery_features, axis=2)\n",
    "    indices = np.argsort(distmat, axis=1)\n",
    "    matches = (gallery_labels[indices] == query_labels[:, np.newaxis])\n",
    "    all_cmc = []\n",
    "    all_AP = []\n",
    "    for i in range(num_q):\n",
    "        valid = matches[i]\n",
    "        if not np.any(valid):\n",
    "            continue\n",
    "        cmc = valid.cumsum()\n",
    "        cmc[cmc > 1] = 1\n",
    "        all_cmc.append(cmc[:max_rank])\n",
    "        num_rel = valid.sum()\n",
    "        tmp_cmc = valid.cumsum() / (np.arange(len(valid)) + 1)\n",
    "        AP = (tmp_cmc * valid).sum() / num_rel\n",
    "        all_AP.append(AP)\n",
    "    cmc = np.mean(all_cmc, axis=0)\n",
    "    mAP = np.mean(all_AP)\n",
    "    return cmc, mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae5eea4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T20:13:03.473114Z",
     "iopub.status.busy": "2025-04-15T20:13:03.472659Z",
     "iopub.status.idle": "2025-04-15T21:10:33.534018Z",
     "shell.execute_reply": "2025-04-15T21:10:33.533052Z"
    },
    "papermill": {
     "duration": 3450.078446,
     "end_time": "2025-04-15T21:10:33.544050",
     "exception": false,
     "start_time": "2025-04-15T20:13:03.465604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/timm/models/_factory.py:126: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb3baec131747edbd8209da4b92e714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 Train Loss: 16.9182 | Val Loss: 13.6485\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 2/50 Train Loss: 12.2833 | Val Loss: 11.6409\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 3/50 Train Loss: 10.5799 | Val Loss: 10.5901\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 4/50 Train Loss: 9.4165 | Val Loss: 9.5804\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 5/50 Train Loss: 8.3642 | Val Loss: 8.6406\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 6/50 Train Loss: 7.4029 | Val Loss: 7.7701\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 7/50 Train Loss: 6.5355 | Val Loss: 7.0415\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 8/50 Train Loss: 5.7643 | Val Loss: 6.5363\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 9/50 Train Loss: 5.1286 | Val Loss: 5.9228\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 10/50 Train Loss: 4.5651 | Val Loss: 5.4222\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 11/50 Train Loss: 4.0489 | Val Loss: 4.9967\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 12/50 Train Loss: 3.6502 | Val Loss: 4.7703\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 13/50 Train Loss: 3.2647 | Val Loss: 4.4076\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 14/50 Train Loss: 2.9630 | Val Loss: 4.1611\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 15/50 Train Loss: 2.6789 | Val Loss: 3.9354\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 16/50 Train Loss: 2.4323 | Val Loss: 3.7088\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 17/50 Train Loss: 2.2122 | Val Loss: 3.5348\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 18/50 Train Loss: 2.0294 | Val Loss: 3.3823\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 19/50 Train Loss: 1.8507 | Val Loss: 3.2387\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 20/50 Train Loss: 1.8257 | Val Loss: 3.2890\n",
      "No improvement for 1 epoch(s).\n",
      "Epoch 21/50 Train Loss: 1.5987 | Val Loss: 3.1182\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 22/50 Train Loss: 1.4763 | Val Loss: 2.9798\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 23/50 Train Loss: 1.3973 | Val Loss: 3.5391\n",
      "No improvement for 1 epoch(s).\n",
      "Epoch 24/50 Train Loss: 1.3353 | Val Loss: 3.2277\n",
      "No improvement for 2 epoch(s).\n",
      "Epoch 25/50 Train Loss: 1.2337 | Val Loss: 2.8996\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 26/50 Train Loss: 1.1244 | Val Loss: 2.8719\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 27/50 Train Loss: 1.0939 | Val Loss: 2.8362\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 28/50 Train Loss: 0.9820 | Val Loss: 2.8281\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 29/50 Train Loss: 0.9515 | Val Loss: 2.9146\n",
      "No improvement for 1 epoch(s).\n",
      "Epoch 30/50 Train Loss: 0.8730 | Val Loss: 2.6546\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 31/50 Train Loss: 0.7968 | Val Loss: 2.7592\n",
      "No improvement for 1 epoch(s).\n",
      "Epoch 32/50 Train Loss: 0.7836 | Val Loss: 2.8598\n",
      "No improvement for 2 epoch(s).\n",
      "Epoch 33/50 Train Loss: 0.7028 | Val Loss: 2.7558\n",
      "No improvement for 3 epoch(s).\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# 7. Putting It All Together: Training with Early Stopping\n",
    "#############################################\n",
    "num_classes = 751  # Adjust as needed.\n",
    "batch_size = 8\n",
    "num_epochs = 50\n",
    "patience = 3\n",
    "\n",
    "# Build the full training dataset with separate transforms.\n",
    "full_train_dataset = Market1501Dataset(\n",
    "    root='/kaggle/input/market-1501/Market-1501-v15.09.15/bounding_box_train/',\n",
    "    transform_eff=train_transforms_eff,\n",
    "    transform_clip=train_transforms_clip\n",
    ")\n",
    "\n",
    "# Split into training and validation sets.\n",
    "num_total = len(full_train_dataset)\n",
    "num_val = int(0.2 * num_total)\n",
    "num_train_new = num_total - num_val\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [num_train_new, num_val])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Test dataset with separate transforms.\n",
    "test_dataset = Market1501Dataset(\n",
    "    root='/kaggle/input/market-1501/Market-1501-v15.09.15/bounding_box_test',\n",
    "    transform_eff=test_transforms_eff,\n",
    "    transform_clip=test_transforms_clip\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize the multi-model.\n",
    "model = MultiModelReID(num_classes=num_classes, parts=3, device=device)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "criterion_tri = BatchHardTripletLoss(margin=0.3)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_multi(model, train_loader, optimizer, criterion_ce, criterion_tri, device)\n",
    "    val_loss = validate(model, val_loader, criterion_ce, criterion_tri, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(\"Validation loss improved. Saving model.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement for {epochs_no_improve} epoch(s).\")\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cf2965d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T21:10:33.563115Z",
     "iopub.status.busy": "2025-04-15T21:10:33.562817Z",
     "iopub.status.idle": "2025-04-15T21:13:17.938644Z",
     "shell.execute_reply": "2025-04-15T21:13:17.937818Z"
    },
    "papermill": {
     "duration": 164.386798,
     "end_time": "2025-04-15T21:13:17.940004",
     "exception": false,
     "start_time": "2025-04-15T21:10:33.553206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/2782254898.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-ranked distance matrix shape: (500, 500)\n",
      "Rank-1 accuracy: 100.00%\n",
      "mAP: 72.48%\n",
      "CMC curve: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# 8. Evaluation and Feature Extraction\n",
    "#############################################\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "query_features, query_labels = extract_features_multi(model, test_loader, device)\n",
    "gallery_features, gallery_labels = extract_features_multi(model, test_loader, device)\n",
    "\n",
    "# Optionally, restrict evaluation to a subset.\n",
    "query_features = query_features[:500]\n",
    "gallery_features = gallery_features[:500]\n",
    "query_labels = query_labels[:500]\n",
    "gallery_labels = gallery_labels[:500]\n",
    "\n",
    "final_dist_matrix = re_ranking(query_features, gallery_features)\n",
    "print(\"Re-ranked distance matrix shape:\", final_dist_matrix.shape)\n",
    "\n",
    "cmc, mAP = evaluate_rank(query_features, query_labels, gallery_features, gallery_labels)\n",
    "print('Rank-1 accuracy: {:.2%}'.format(cmc[0]))\n",
    "print('mAP: {:.2%}'.format(mAP))\n",
    "print(\"CMC curve:\", cmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b723436",
   "metadata": {
    "papermill": {
     "duration": 0.009133,
     "end_time": "2025-04-15T21:13:17.959282",
     "exception": false,
     "start_time": "2025-04-15T21:13:17.950149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 39965,
     "sourceId": 62075,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3774.937987,
   "end_time": "2025-04-15T21:13:21.002762",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-15T20:10:26.064775",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1261d1e247104c9ea96fa94dd077c619": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1631baf5e29f47618a26d57c51b1eba2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_496516b9817b4db2b3492dcb306e0977",
       "max": 21355344.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_861087b435c84bdc87344e6091522f0f",
       "tabbable": null,
       "tooltip": null,
       "value": 21355344.0
      }
     },
     "26d0b0e10ce24df2b880fa0200bc0488": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "41ba256765a34aad9a5e23383c9d5b8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "496516b9817b4db2b3492dcb306e0977": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5c1f6bc1b7dd49508d2ca31e95aad1a0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "707d9f1b8a2c4a0c89ffe35b754cda28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a169111109fb4a9995695d36f2533b7d",
       "placeholder": "​",
       "style": "IPY_MODEL_41ba256765a34aad9a5e23383c9d5b8e",
       "tabbable": null,
       "tooltip": null,
       "value": " 21.4M/21.4M [00:00&lt;00:00, 97.4MB/s]"
      }
     },
     "861087b435c84bdc87344e6091522f0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a169111109fb4a9995695d36f2533b7d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ad24b70b6d9f4b5f92775aac309e1dc1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1261d1e247104c9ea96fa94dd077c619",
       "placeholder": "​",
       "style": "IPY_MODEL_26d0b0e10ce24df2b880fa0200bc0488",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "dbb3baec131747edbd8209da4b92e714": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ad24b70b6d9f4b5f92775aac309e1dc1",
        "IPY_MODEL_1631baf5e29f47618a26d57c51b1eba2",
        "IPY_MODEL_707d9f1b8a2c4a0c89ffe35b754cda28"
       ],
       "layout": "IPY_MODEL_5c1f6bc1b7dd49508d2ca31e95aad1a0",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
